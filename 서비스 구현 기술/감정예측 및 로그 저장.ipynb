{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b94d3ec3-8fda-47b8-a3f6-5afd5814dac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\pyhon-workspace\\1ì¸ ê°€êµ¬\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "535ae4dd-88b6-4c78-b294-051b94dd1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import aihub_embedding\n",
    "print(dir(aihub_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6e4085-7de2-4e44-9037-c9d8002dcd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asia\\.conda\\envs\\py312tf9\\Lib\\site-packages\\transformers\\configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Wav2Vec2 ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© í•¨ìˆ˜ ì •ì˜\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ì˜¤ë””ì˜¤ ë¡œë”© í•¨ìˆ˜\n",
    "def load_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform.squeeze()\n",
    "\n",
    "# ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_embedding(wav_path):\n",
    "    try:\n",
    "        waveform = load_audio(wav_path)\n",
    "        inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs.to(device))\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {wav_path}, {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9de34b-b584-4716-9ff9-a16e54b98a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['.ipynb_checkpoints', '15ì´ˆ ë‹¨ìœ„ audio chunk ì €ì¥í•˜ê¸°.ipynb', 'aihub ë³‘í•© ë° ë§µí•‘.ipynb', 'aihub ì„ë² ë”©.ipynb', 'aihub í•™ìŠµ ë° í‰ê°€ .ipynb', 'aihub_embedding.py', 'aihub_emotion_preprocessed_cleaned.csv', 'brighter-datasetBRIGHTER-emotion-categories.ipynb', 'diarized_chunks', 'EmoGator-main', 'EmoGatormp32wav.ipynb', 'EmoGator_metadata.csv', 'emotion_classifier.pkl', 'emotion_classifier.pth', 'emotion_faiss.index', 'emotion_label_encoder.pkl', 'emotion_X.npy', 'emotion_X_no_disgust.npy', 'emotion_y.npy', 'emotion_y_no_disgust.npy', 'faiss ì¸ë±ìŠ¤ í™œìš© ë°©ì‹ ìš”ì•½.ipynb', 'faiss.ipynb', 'faiss_metadata.pkl', 'label_encoder_classes.npy', 'madecsv.ipynb', 'makecsv.py', 'temp_chunks', 'user_reference', 'user_segments_merged.wav', 'wav mapping check.ipynb', 'wav2vec2_embeddings.pkl', '__pycache__', 'ê°ì • ë¶„ë¥˜ë¥¼ ìœ„í•œ ëŒ€í™” ìŒì„± ë°ì´í„°ì…‹', 'ê°ì •ì˜ˆì¸¡ ë° ë¡œê·¸ ì €ì¥.ipynb', 'ë©€í‹°ë¼ë²¨ ë²¡í„°í™” + ë°ì´í„°ì…‹ ì¤€ë¹„.ipynb', 'ì‹ í˜¸ ì „ì²˜ë¦¬.ipynb', 'ì„ë² ë”© ì½”ë“œ.ipynb', 'ìë£Œ', 'í™”ì ë¶„ë¥˜ ì½”ë“œ.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ì˜ˆì‹œ: ë‹¨ì¼ ì˜ˆì¸¡ì„ ìœ„í•´ ì‚¬ìš©í•  ì˜¤ë””ì˜¤ ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ë¡œ ì§€ì •\n",
    "merged_path = os.path.join(\"ê°ì • ë¶„ë¥˜ë¥¼ ìœ„í•œ ëŒ€í™” ìŒì„± ë°ì´í„°ì…‹\", \"4ì°¨ë…„ë„\", \"your_audio_file.wav\")\n",
    "\n",
    "# ì‹¤ì œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "print(os.path.exists(merged_path))  # Trueì´ë©´ OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20059bc-3837-492d-b101-151ecf12bd07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_embedding' from 'aihub_embedding' (D:\\pyhon-workspace\\soundofmind\\aihub_embedding.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 5. ìŒì„± ì„ë² ë”© í•¨ìˆ˜ (ê¸°ì¡´ wav2vec2 ê¸°ë°˜ í•¨ìˆ˜ í˜¸ì¶œ í•„ìš”)\u001b[39;00m\n\u001b[32m     34\u001b[39m sys.path.append(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maihub_embedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_embedding  \u001b[38;5;66;03m# ì´ë¯¸ ì‘ì„±ëœ ì„ë² ë”© í•¨ìˆ˜\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# ì˜ˆì¸¡ ëŒ€ìƒ ì˜¤ë””ì˜¤ ê²½ë¡œ\u001b[39;00m\n\u001b[32m     38\u001b[39m merged_path = \u001b[33m\"\u001b[39m\u001b[33m./user_segments_merged.wav\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'extract_embedding' from 'aihub_embedding' (D:\\pyhon-workspace\\soundofmind\\aihub_embedding.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# 1. MLP ëª¨ë¸ êµ¬ì¡° ì •ì˜ (í•™ìŠµ ë‹¹ì‹œì™€ ë™ì¼í•´ì•¼ í•¨)\n",
    "class EmotionMLP(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 2. ê°ì • ë¼ë²¨ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìˆœì„œì™€ ë™ì¼í•˜ê²Œ)\n",
    "emotion_labels = ['anger', 'happiness', 'neutral', 'sadness']\n",
    "\n",
    "# 3. ì¥ì¹˜ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 4. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "emotion_model = EmotionMLP().to(device)\n",
    "emotion_model.load_state_dict(torch.load(\"emotion_classifier.pth\", map_location=device))\n",
    "emotion_model.eval()\n",
    "\n",
    "# 5. ìŒì„± ì„ë² ë”© í•¨ìˆ˜ (ê¸°ì¡´ wav2vec2 ê¸°ë°˜ í•¨ìˆ˜ í˜¸ì¶œ í•„ìš”)\n",
    "sys.path.append(\".\")\n",
    "from aihub_embedding import extract_embedding  # ì´ë¯¸ ì‘ì„±ëœ ì„ë² ë”© í•¨ìˆ˜\n",
    "\n",
    "# ì˜ˆì¸¡ ëŒ€ìƒ ì˜¤ë””ì˜¤ ê²½ë¡œ\n",
    "merged_path = \"./user_segments_merged.wav\"\n",
    "\n",
    "# 6. ê°ì • ì˜ˆì¸¡\n",
    "with torch.no_grad():\n",
    "    embedding = extract_embedding(merged_path)  # shape: (768,)\n",
    "    tensor_input = torch.tensor(embedding).unsqueeze(0).to(device).float()\n",
    "    logits = emotion_model(tensor_input)\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "    predicted_emotion = emotion_labels[pred]\n",
    "\n",
    "# 7. ë¡œê·¸ ì €ì¥\n",
    "log_df = pd.DataFrame([{\n",
    "    \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"file\": os.path.basename(merged_path),\n",
    "    \"predicted_emotion\": predicted_emotion\n",
    "}])\n",
    "log_df.to_csv(\"emotion_log.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… ì˜ˆì¸¡ëœ ê°ì •: {predicted_emotion}\")\n",
    "print(\"ğŸ“„ emotion_log.csvì— ì €ì¥ ì™„ë£Œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
