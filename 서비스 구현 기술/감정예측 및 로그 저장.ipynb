{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b94d3ec3-8fda-47b8-a3f6-5afd5814dac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\pyhon-workspace\\1인 가구\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "535ae4dd-88b6-4c78-b294-051b94dd1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import aihub_embedding\n",
    "print(dir(aihub_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6e4085-7de2-4e44-9037-c9d8002dcd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asia\\.conda\\envs\\py312tf9\\Lib\\site-packages\\transformers\\configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Wav2Vec2 모델 로드 및 임베딩 함수 정의\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# 모델 불러오기\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 오디오 로딩 함수\n",
    "def load_audio(file_path):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform.squeeze()\n",
    "\n",
    "# 임베딩 추출 함수\n",
    "def extract_embedding(wav_path):\n",
    "    try:\n",
    "        waveform = load_audio(wav_path)\n",
    "        inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs.to(device))\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류: {wav_path}, {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9de34b-b584-4716-9ff9-a16e54b98a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['.ipynb_checkpoints', '15초 단위 audio chunk 저장하기.ipynb', 'aihub 병합 및 맵핑.ipynb', 'aihub 임베딩.ipynb', 'aihub 학습 및 평가 .ipynb', 'aihub_embedding.py', 'aihub_emotion_preprocessed_cleaned.csv', 'brighter-datasetBRIGHTER-emotion-categories.ipynb', 'diarized_chunks', 'EmoGator-main', 'EmoGatormp32wav.ipynb', 'EmoGator_metadata.csv', 'emotion_classifier.pkl', 'emotion_classifier.pth', 'emotion_faiss.index', 'emotion_label_encoder.pkl', 'emotion_X.npy', 'emotion_X_no_disgust.npy', 'emotion_y.npy', 'emotion_y_no_disgust.npy', 'faiss 인덱스 활용 방식 요약.ipynb', 'faiss.ipynb', 'faiss_metadata.pkl', 'label_encoder_classes.npy', 'madecsv.ipynb', 'makecsv.py', 'temp_chunks', 'user_reference', 'user_segments_merged.wav', 'wav mapping check.ipynb', 'wav2vec2_embeddings.pkl', '__pycache__', '감정 분류를 위한 대화 음성 데이터셋', '감정예측 및 로그 저장.ipynb', '멀티라벨 벡터화 + 데이터셋 준비.ipynb', '신호 전처리.ipynb', '임베딩 코드.ipynb', '자료', '화자 분류 코드.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 예시: 단일 예측을 위해 사용할 오디오 경로를 실제 파일로 지정\n",
    "merged_path = os.path.join(\"감정 분류를 위한 대화 음성 데이터셋\", \"4차년도\", \"your_audio_file.wav\")\n",
    "\n",
    "# 실제 존재하는지 확인\n",
    "print(os.path.exists(merged_path))  # True이면 OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20059bc-3837-492d-b101-151ecf12bd07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_embedding' from 'aihub_embedding' (D:\\pyhon-workspace\\soundofmind\\aihub_embedding.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 5. 음성 임베딩 함수 (기존 wav2vec2 기반 함수 호출 필요)\u001b[39;00m\n\u001b[32m     34\u001b[39m sys.path.append(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maihub_embedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_embedding  \u001b[38;5;66;03m# 이미 작성된 임베딩 함수\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# 예측 대상 오디오 경로\u001b[39;00m\n\u001b[32m     38\u001b[39m merged_path = \u001b[33m\"\u001b[39m\u001b[33m./user_segments_merged.wav\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'extract_embedding' from 'aihub_embedding' (D:\\pyhon-workspace\\soundofmind\\aihub_embedding.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# 1. MLP 모델 구조 정의 (학습 당시와 동일해야 함)\n",
    "class EmotionMLP(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 2. 감정 라벨 (학습 시 사용한 순서와 동일하게)\n",
    "emotion_labels = ['anger', 'happiness', 'neutral', 'sadness']\n",
    "\n",
    "# 3. 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 4. 모델 불러오기\n",
    "emotion_model = EmotionMLP().to(device)\n",
    "emotion_model.load_state_dict(torch.load(\"emotion_classifier.pth\", map_location=device))\n",
    "emotion_model.eval()\n",
    "\n",
    "# 5. 음성 임베딩 함수 (기존 wav2vec2 기반 함수 호출 필요)\n",
    "sys.path.append(\".\")\n",
    "from aihub_embedding import extract_embedding  # 이미 작성된 임베딩 함수\n",
    "\n",
    "# 예측 대상 오디오 경로\n",
    "merged_path = \"./user_segments_merged.wav\"\n",
    "\n",
    "# 6. 감정 예측\n",
    "with torch.no_grad():\n",
    "    embedding = extract_embedding(merged_path)  # shape: (768,)\n",
    "    tensor_input = torch.tensor(embedding).unsqueeze(0).to(device).float()\n",
    "    logits = emotion_model(tensor_input)\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "    predicted_emotion = emotion_labels[pred]\n",
    "\n",
    "# 7. 로그 저장\n",
    "log_df = pd.DataFrame([{\n",
    "    \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"file\": os.path.basename(merged_path),\n",
    "    \"predicted_emotion\": predicted_emotion\n",
    "}])\n",
    "log_df.to_csv(\"emotion_log.csv\", index=False)\n",
    "\n",
    "print(f\"✅ 예측된 감정: {predicted_emotion}\")\n",
    "print(\"📄 emotion_log.csv에 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
