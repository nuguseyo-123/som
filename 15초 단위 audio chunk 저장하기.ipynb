{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05aa3e6a-8b89-413a-88c7-0e0c5a44a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "from pyannote.audio import Pipeline\n",
    "import warnings\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchaudio.pipelines import WAV2VEC2_BASE\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1c5202-7076-4b01-8186-33fde20e6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15초 단위 audio chunk 저장하기, 저장경로 지정(음성 입력)\n",
    "def save_audio_chunks(waveform, sr, chunk_sec=15, save_dir=\"temp_chunks\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    chunk_len = chunk_sec * sr\n",
    "    total_len = waveform.shape[1]\n",
    "    \n",
    "    for i, start in enumerate(range(0, total_len, chunk_len)):\n",
    "        end = min(start + chunk_len, total_len)\n",
    "        chunk = waveform[:, start:end]\n",
    "        out_path = os.path.join(save_dir, f\"chunk_{i:03d}.wav\")\n",
    "        sf.write(out_path, chunk.T, sr)  # chunk.T to shape (samples, channels)\n",
    "        print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af53399-ab41-4897-81c9-3685eae2e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asia\\.conda\\envs\\py312tf9\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Diarizing: ./temp_chunks\\test.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asia\\.conda\\envs\\py312tf9\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.wav: 0.2–0.5s speaker_SPEAKER_01\n",
      "test.wav: 1.2–1.7s speaker_SPEAKER_01\n",
      "test.wav: 2.3–10.5s speaker_SPEAKER_00\n",
      "test.wav: 11.3–11.7s speaker_SPEAKER_02\n"
     ]
    }
   ],
   "source": [
    "# diarization 실행 코드\n",
    "HF_TOKEN = 'hf'\n",
    "\n",
    "#pipeline 로드\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "chunk_dir =  \"./temp_chunks\"\n",
    "\n",
    "#diarization 수행\n",
    "for fname in sorted(os.listdir(chunk_dir)):\n",
    "    if not fname.endswith(\".wav\"):\n",
    "        continue\n",
    "    file_path = os.path.join(chunk_dir, fname)\n",
    "    print(f\"▶️ Diarizing: {file_path}\")\n",
    "\n",
    "    diarization = pipeline(file_path)\n",
    "\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        print(f\"{fname}: {turn.start:.1f}–{turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea790d68-c8cd-4035-8722-749a1cf7e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 파일이 존재하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"user_embedding.npy\"):\n",
    "    os.remove(\"user_embedding.npy\")\n",
    "    print(\"✅ user_embedding.npy 삭제 완료\")\n",
    "else:\n",
    "    print(\"❌ 파일이 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2539072-78dd-4c4b-b4df-0d3d60bd7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용자 임베딩 저장\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "model = bundle.get_model().to(device).eval()\n",
    "\n",
    "def extract_wav2vec2_embedding(path):\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    \n",
    "    # 💡 [중요] 다채널인 경우 평균을 내어 mono로 변환\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # → (1, time)\n",
    "\n",
    "    if sr != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, bundle.sample_rate)\n",
    "\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(waveform)  # (batch=1, time, 768)\n",
    "\n",
    "    emb = out[0].mean(dim=1)  # → (1, 768)\n",
    "\n",
    "    return emb.cpu().numpy()  # → (1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c5e6d37-ade0-4107-a696-71584ed643f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m user_embedding = extract_wav2vec2_embedding(user_path)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ user_embedding.shape:\u001b[39m\u001b[33m\"\u001b[39m, user_embedding.shape)  \u001b[38;5;66;03m# 반드시 (1, 768)이어야 함\u001b[39;00m\n\u001b[32m      3\u001b[39m np.save(\u001b[33m\"\u001b[39m\u001b[33muser_embedding.npy\u001b[39m\u001b[33m\"\u001b[39m, user_embedding)\n",
      "\u001b[31mNameError\u001b[39m: name 'user_path' is not defined"
     ]
    }
   ],
   "source": [
    "user_embedding = extract_wav2vec2_embedding(user_path)\n",
    "print(\"✅ user_embedding.shape:\", user_embedding.shape)  # 반드시 (1, 768)이어야 함\n",
    "np.save(\"user_embedding.npy\", user_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "044baa4b-cdbd-4c6e-8b72-592039ca019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./diarized_chunks\\test_spkSPEAKER_01_000.wav\n",
      "Saved: ./diarized_chunks\\test_spkSPEAKER_01_001.wav\n",
      "Saved: ./diarized_chunks\\test_spkSPEAKER_00_002.wav\n",
      "Saved: ./diarized_chunks\\test_spkSPEAKER_02_003.wav\n"
     ]
    }
   ],
   "source": [
    "chunk_path = \"./temp_chunks/test.wav\"\n",
    "\n",
    "diarization_result = pipeline(chunk_path)\n",
    "output_dir = './diarized_chunks'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "waveform, sr = torchaudio.load(chunk_path)\n",
    "waveform = waveform.squeeze().numpy()\n",
    "\n",
    "for i, (segment, _ , speaker) in enumerate(diarization_result.itertracks(yield_label=True)):\n",
    "    start_sample = int(segment.start * sr)\n",
    "    end_sample = int(segment.end * sr)\n",
    "    segment_wave = waveform[start_sample:end_sample]\n",
    "    out_path = os.path.join(output_dir, f\"{os.path.basename(chunk_path).replace('.wav','')}_spk{speaker}_{i:03d}.wav\")\n",
    "    sf.write(out_path, segment_wave, sr)\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48f3e834-90d6-4461-ba37-7cabb5e98d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Bundle(_path='wav2vec2_fairseq_base_ls960.pth', _params={'extractor_mode': 'group_norm', 'extractor_conv_layer_config': [(512, 10, 5), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 2, 2), (512, 2, 2)], 'extractor_conv_bias': False, 'encoder_embed_dim': 768, 'encoder_projection_dropout': 0.1, 'encoder_pos_conv_kernel': 128, 'encoder_pos_conv_groups': 16, 'encoder_num_layers': 12, 'encoder_num_heads': 12, 'encoder_attention_dropout': 0.1, 'encoder_ff_interm_features': 3072, 'encoder_ff_interm_dropout': 0.0, 'encoder_dropout': 0.1, 'encoder_layer_norm_first': False, 'encoder_layer_drop': 0.05, 'aux_num_out': None}, _sample_rate=16000, _normalize_waveform=False, _model_type='Wav2Vec2')\n"
     ]
    }
   ],
   "source": [
    "print(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a9494dd-1d2e-4fef-bac2-53640cef20b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사용자 발화: test_spkSPEAKER_00_002.wav (similarity: 0.812)\n",
      "❌ 타인 발화: test_spkSPEAKER_01_000.wav (similarity: 0.602)\n",
      "❌ 타인 발화: test_spkSPEAKER_01_001.wav (similarity: 0.557)\n",
      "❌ 타인 발화: test_spkSPEAKER_02_003.wav (similarity: 0.710)\n"
     ]
    }
   ],
   "source": [
    "# 화자 식별 코드\n",
    "#사용자 입데이 불러오기\n",
    "user_embedding = np.load('user_embedding.npy')\n",
    "\n",
    "#diarized_cunks 폴더에서 segment별 음성 비교\n",
    "diarized_dir = './diarized_chunks'\n",
    "user_segments = []\n",
    "similarity_threshold = 0.8 # 유사성 임계값\n",
    "for fname in os.listdir(diarized_dir):\n",
    "    if not fname.endswith(\".wav\"):\n",
    "        continue\n",
    "    path = os.path.join(diarized_dir, fname)\n",
    "    segment_embedding = extract_wav2vec2_embedding(path)\n",
    "\n",
    "    sim = cosine_similarity(user_embedding, segment_embedding)[0][0]\n",
    "\n",
    "    if sim >= similarity_threshold:\n",
    "        print(f\"✅ 사용자 발화: {fname} (similarity: {sim:.3f})\")\n",
    "        user_segments.append(fname)\n",
    "    else:\n",
    "        print(f\"❌ 타인 발화: {fname} (similarity: {sim:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83d4c1b7-bf8d-4375-bc8d-c48b8c40762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사용자 음성 segment 병합 완료: ./user_segments_merged.wav\n"
     ]
    }
   ],
   "source": [
    "# 사용자 발화 병합 및 저장\n",
    "merged_waveform = []\n",
    "sr = bundle.sample_rate  # Wav2Vec2 모델 샘플레이트\n",
    "\n",
    "for fname in sorted(user_segments):  # 시간 순서를 위해 정렬\n",
    "    path = os.path.join(diarized_dir, fname)\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    \n",
    "    # 다채널이면 mono로 변환\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    merged_waveform.append(waveform)\n",
    "\n",
    "# 병합\n",
    "merged_waveform = torch.cat(merged_waveform, dim=1)  # (1, total_samples)\n",
    "\n",
    "# 저장\n",
    "output_path = \"./user_segments_merged.wav\"\n",
    "sf.write(output_path, merged_waveform.squeeze().numpy(), sr)\n",
    "print(f\"✅ 사용자 음성 segment 병합 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00c58ff7-01ef-49ac-88cd-7bef2fedeed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmotionMLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 모델 불러오기\u001b[39;00m\n\u001b[32m     14\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m emotion_model = EmotionMLP().to(device)\n\u001b[32m     16\u001b[39m emotion_model.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33memotion_classifier.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=device))\n\u001b[32m     17\u001b[39m emotion_model.eval()\n",
      "\u001b[31mNameError\u001b[39m: name 'EmotionMLP' is not defined"
     ]
    }
   ],
   "source": [
    "# 임시 감정 분류기\n",
    "class DummyEmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 감정 라벨 정의 (실제 모델에 맞춰 수정)\n",
    "emotion_labels = ['anger', 'happiness', 'neutral', 'sadness']\n",
    "\n",
    "# 모델 불러오기\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_model = EmotionMLP().to(device)\n",
    "emotion_model.load_state_dict(torch.load(\"emotion_classifier.pth\", map_location=device))\n",
    "emotion_model.eval()\n",
    "\n",
    "# 오디오에서 임베딩 추출\n",
    "merged_path = \"./user_segments_merged.wav\"\n",
    "embedding = extract_wav2vec2_embedding(merged_path)  # (1, 768)\n",
    "\n",
    "# 감정 예측\n",
    "with torch.no_grad():\n",
    "    logits = emotion_model(torch.from_numpy(embedding).to(device).float())\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "    emotion = emotion_labels[pred]\n",
    "\n",
    "# 결과 저장\n",
    "df = pd.DataFrame([{\n",
    "    \"timestamp\": pd.Timestamp.now(),\n",
    "    \"file\": os.path.basename(merged_path),\n",
    "    \"predicted_emotion\": emotion\n",
    "}])\n",
    "df.to_csv(\"emotion_log.csv\", index=False)\n",
    "\n",
    "print(f\"🎯 예측된 감정: {emotion}\")\n",
    "print(\"📄 emotion_log.csv에 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ad3ea-d369-4ce6-8640-9ab377bcfd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
