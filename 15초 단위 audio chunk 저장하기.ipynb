{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05aa3e6a-8b89-413a-88c7-0e0c5a44a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "from pyannote.audio import Pipeline\n",
    "import warnings\n",
    "import torchaudio\n",
    "import torch\n",
    "from torchaudio.pipelines import WAV2VEC2_BASE\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1c5202-7076-4b01-8186-33fde20e6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15ì´ˆ ë‹¨ìœ„ audio chunk ì €ì¥í•˜ê¸°, ì €ì¥ê²½ë¡œ ì§€ì •(ìŒì„± ì…ë ¥)\n",
    "def save_audio_chunks(waveform, sr, chunk_sec=15, save_dir=\"temp_chunks\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    chunk_len = chunk_sec * sr\n",
    "    total_len = waveform.shape[1]\n",
    "    \n",
    "    for i, start in enumerate(range(0, total_len, chunk_len)):\n",
    "        end = min(start + chunk_len, total_len)\n",
    "        chunk = waveform[:, start:end]\n",
    "        out_path = os.path.join(save_dir, f\"chunk_{i:03d}.wav\")\n",
    "        sf.write(out_path, chunk.T, sr)  # chunk.T to shape (samples, channels)\n",
    "        print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af53399-ab41-4897-81c9-3685eae2e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asia\\.conda\\envs\\py312tf9\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ Diarizing: ./temp_chunks\\test.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asia\\.conda\\envs\\py312tf9\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.wav: 0.2â€“0.5s speaker_SPEAKER_01\n",
      "test.wav: 1.2â€“1.7s speaker_SPEAKER_01\n",
      "test.wav: 2.3â€“10.5s speaker_SPEAKER_00\n",
      "test.wav: 11.3â€“11.7s speaker_SPEAKER_02\n"
     ]
    }
   ],
   "source": [
    "# diarization ì‹¤í–‰ ì½”ë“œ\n",
    "HF_TOKEN = 'hf'\n",
    "\n",
    "#pipeline ë¡œë“œ\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "chunk_dir =  \"./temp_chunks\"\n",
    "\n",
    "#diarization ìˆ˜í–‰\n",
    "for fname in sorted(os.listdir(chunk_dir)):\n",
    "    if not fname.endswith(\".wav\"):\n",
    "        continue\n",
    "    file_path = os.path.join(chunk_dir, fname)\n",
    "    print(f\"â–¶ï¸ Diarizing: {file_path}\")\n",
    "\n",
    "    diarization = pipeline(file_path)\n",
    "\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        print(f\"{fname}: {turn.start:.1f}â€“{turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea790d68-c8cd-4035-8722-749a1cf7e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"user_embedding.npy\"):\n",
    "    os.remove(\"user_embedding.npy\")\n",
    "    print(\"âœ… user_embedding.npy ì‚­ì œ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2539072-78dd-4c4b-b4df-0d3d60bd7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì‚¬ìš©ì ì„ë² ë”© ì €ì¥\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "model = bundle.get_model().to(device).eval()\n",
    "\n",
    "def extract_wav2vec2_embedding(path):\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    \n",
    "    # ğŸ’¡ [ì¤‘ìš”] ë‹¤ì±„ë„ì¸ ê²½ìš° í‰ê· ì„ ë‚´ì–´ monoë¡œ ë³€í™˜\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # â†’ (1, time)\n",
    "\n",
    "    if sr != bundle.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, bundle.sample_rate)\n",
    "\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(waveform)  # (batch=1, time, 768)\n",
    "\n",
    "    emb = out[0].mean(dim=1)  # â†’ (1, 768)\n",
    "\n",
    "    return emb.cpu().numpy()  # â†’ (1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c5e6d37-ade0-4107-a696-71584ed643f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m user_embedding = extract_wav2vec2_embedding(user_path)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… user_embedding.shape:\u001b[39m\u001b[33m\"\u001b[39m, user_embedding.shape)  \u001b[38;5;66;03m# ë°˜ë“œì‹œ (1, 768)ì´ì–´ì•¼ í•¨\u001b[39;00m\n\u001b[32m      3\u001b[39m np.save(\u001b[33m\"\u001b[39m\u001b[33muser_embedding.npy\u001b[39m\u001b[33m\"\u001b[39m, user_embedding)\n",
      "\u001b[31mNameError\u001b[39m: name 'user_path' is not defined"
     ]
    }
   ],
   "source": [
    "user_embedding = extract_wav2vec2_embedding(user_path)\n",
    "print(\"âœ… user_embedding.shape:\", user_embedding.shape)  # ë°˜ë“œì‹œ (1, 768)ì´ì–´ì•¼ í•¨\n",
    "np.save(\"user_embedding.npy\", user_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "044baa4b-cdbd-4c6e-8b72-592039ca019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./diarized_chunks\\test_spkSPEAKER_01_000.wav\n",
      "Saved: ./diarized_chunks\\test_spkSPEAKER_01_001.wav\n",
      "Saved: ./diarized_chunks\\test_spkSPEAKER_00_002.wav\n",
      "Saved: ./diarized_chunks\\test_spkSPEAKER_02_003.wav\n"
     ]
    }
   ],
   "source": [
    "chunk_path = \"./temp_chunks/test.wav\"\n",
    "\n",
    "diarization_result = pipeline(chunk_path)\n",
    "output_dir = './diarized_chunks'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "waveform, sr = torchaudio.load(chunk_path)\n",
    "waveform = waveform.squeeze().numpy()\n",
    "\n",
    "for i, (segment, _ , speaker) in enumerate(diarization_result.itertracks(yield_label=True)):\n",
    "    start_sample = int(segment.start * sr)\n",
    "    end_sample = int(segment.end * sr)\n",
    "    segment_wave = waveform[start_sample:end_sample]\n",
    "    out_path = os.path.join(output_dir, f\"{os.path.basename(chunk_path).replace('.wav','')}_spk{speaker}_{i:03d}.wav\")\n",
    "    sf.write(out_path, segment_wave, sr)\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48f3e834-90d6-4461-ba37-7cabb5e98d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Bundle(_path='wav2vec2_fairseq_base_ls960.pth', _params={'extractor_mode': 'group_norm', 'extractor_conv_layer_config': [(512, 10, 5), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 2, 2), (512, 2, 2)], 'extractor_conv_bias': False, 'encoder_embed_dim': 768, 'encoder_projection_dropout': 0.1, 'encoder_pos_conv_kernel': 128, 'encoder_pos_conv_groups': 16, 'encoder_num_layers': 12, 'encoder_num_heads': 12, 'encoder_attention_dropout': 0.1, 'encoder_ff_interm_features': 3072, 'encoder_ff_interm_dropout': 0.0, 'encoder_dropout': 0.1, 'encoder_layer_norm_first': False, 'encoder_layer_drop': 0.05, 'aux_num_out': None}, _sample_rate=16000, _normalize_waveform=False, _model_type='Wav2Vec2')\n"
     ]
    }
   ],
   "source": [
    "print(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a9494dd-1d2e-4fef-bac2-53640cef20b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‚¬ìš©ì ë°œí™”: test_spkSPEAKER_00_002.wav (similarity: 0.812)\n",
      "âŒ íƒ€ì¸ ë°œí™”: test_spkSPEAKER_01_000.wav (similarity: 0.602)\n",
      "âŒ íƒ€ì¸ ë°œí™”: test_spkSPEAKER_01_001.wav (similarity: 0.557)\n",
      "âŒ íƒ€ì¸ ë°œí™”: test_spkSPEAKER_02_003.wav (similarity: 0.710)\n"
     ]
    }
   ],
   "source": [
    "# í™”ì ì‹ë³„ ì½”ë“œ\n",
    "#ì‚¬ìš©ì ì…ë°ì´ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "user_embedding = np.load('user_embedding.npy')\n",
    "\n",
    "#diarized_cunks í´ë”ì—ì„œ segmentë³„ ìŒì„± ë¹„êµ\n",
    "diarized_dir = './diarized_chunks'\n",
    "user_segments = []\n",
    "similarity_threshold = 0.8 # ìœ ì‚¬ì„± ì„ê³„ê°’\n",
    "for fname in os.listdir(diarized_dir):\n",
    "    if not fname.endswith(\".wav\"):\n",
    "        continue\n",
    "    path = os.path.join(diarized_dir, fname)\n",
    "    segment_embedding = extract_wav2vec2_embedding(path)\n",
    "\n",
    "    sim = cosine_similarity(user_embedding, segment_embedding)[0][0]\n",
    "\n",
    "    if sim >= similarity_threshold:\n",
    "        print(f\"âœ… ì‚¬ìš©ì ë°œí™”: {fname} (similarity: {sim:.3f})\")\n",
    "        user_segments.append(fname)\n",
    "    else:\n",
    "        print(f\"âŒ íƒ€ì¸ ë°œí™”: {fname} (similarity: {sim:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83d4c1b7-bf8d-4375-bc8d-c48b8c40762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì‚¬ìš©ì ìŒì„± segment ë³‘í•© ì™„ë£Œ: ./user_segments_merged.wav\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ì ë°œí™” ë³‘í•© ë° ì €ì¥\n",
    "merged_waveform = []\n",
    "sr = bundle.sample_rate  # Wav2Vec2 ëª¨ë¸ ìƒ˜í”Œë ˆì´íŠ¸\n",
    "\n",
    "for fname in sorted(user_segments):  # ì‹œê°„ ìˆœì„œë¥¼ ìœ„í•´ ì •ë ¬\n",
    "    path = os.path.join(diarized_dir, fname)\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    \n",
    "    # ë‹¤ì±„ë„ì´ë©´ monoë¡œ ë³€í™˜\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    merged_waveform.append(waveform)\n",
    "\n",
    "# ë³‘í•©\n",
    "merged_waveform = torch.cat(merged_waveform, dim=1)  # (1, total_samples)\n",
    "\n",
    "# ì €ì¥\n",
    "output_path = \"./user_segments_merged.wav\"\n",
    "sf.write(output_path, merged_waveform.squeeze().numpy(), sr)\n",
    "print(f\"âœ… ì‚¬ìš©ì ìŒì„± segment ë³‘í•© ì™„ë£Œ: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00c58ff7-01ef-49ac-88cd-7bef2fedeed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmotionMLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\u001b[39;00m\n\u001b[32m     14\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m emotion_model = EmotionMLP().to(device)\n\u001b[32m     16\u001b[39m emotion_model.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33memotion_classifier.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=device))\n\u001b[32m     17\u001b[39m emotion_model.eval()\n",
      "\u001b[31mNameError\u001b[39m: name 'EmotionMLP' is not defined"
     ]
    }
   ],
   "source": [
    "# ì„ì‹œ ê°ì • ë¶„ë¥˜ê¸°\n",
    "class DummyEmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# ê°ì • ë¼ë²¨ ì •ì˜ (ì‹¤ì œ ëª¨ë¸ì— ë§ì¶° ìˆ˜ì •)\n",
    "emotion_labels = ['anger', 'happiness', 'neutral', 'sadness']\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_model = EmotionMLP().to(device)\n",
    "emotion_model.load_state_dict(torch.load(\"emotion_classifier.pth\", map_location=device))\n",
    "emotion_model.eval()\n",
    "\n",
    "# ì˜¤ë””ì˜¤ì—ì„œ ì„ë² ë”© ì¶”ì¶œ\n",
    "merged_path = \"./user_segments_merged.wav\"\n",
    "embedding = extract_wav2vec2_embedding(merged_path)  # (1, 768)\n",
    "\n",
    "# ê°ì • ì˜ˆì¸¡\n",
    "with torch.no_grad():\n",
    "    logits = emotion_model(torch.from_numpy(embedding).to(device).float())\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "    emotion = emotion_labels[pred]\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df = pd.DataFrame([{\n",
    "    \"timestamp\": pd.Timestamp.now(),\n",
    "    \"file\": os.path.basename(merged_path),\n",
    "    \"predicted_emotion\": emotion\n",
    "}])\n",
    "df.to_csv(\"emotion_log.csv\", index=False)\n",
    "\n",
    "print(f\"ğŸ¯ ì˜ˆì¸¡ëœ ê°ì •: {emotion}\")\n",
    "print(\"ğŸ“„ emotion_log.csvì— ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ad3ea-d369-4ce6-8640-9ab377bcfd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
